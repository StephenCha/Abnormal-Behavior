{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af3b446a-f36f-46ff-b4b7-9dca78edbe14",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 판교 AI Challenge\n",
    "> 참치김치찌개팀<br>\n",
    "> 팀장 손찬영, 팀원 김민정 김하림 이두현 차현수\n",
    "* 과제명 : [아동 및 교통약자 보호를 위한 어린이 도로보행 위험행동 분류 과제]\n",
    "* 과제 링크 : https://www.aiconnect.kr/main/competition/privateDetail/200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c399d1e-7303-4345-a7ae-2cb65b7dd813",
   "metadata": {
    "tags": []
   },
   "source": [
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6342062d-42a6-49df-a478-880e685c1d1b",
   "metadata": {},
   "source": [
    "## Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30321103-75de-4d8d-815e-c066da74a9ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import timeit\n",
    "import warnings\n",
    "\n",
    "import easydict\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Others\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Customized Source Python Files\n",
    "import source.dataset as dataset\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "from source.model import C3D_model, R2Plus1D_model, R3D_model\n",
    "from source.model.utils.vit import TimeSformer\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dcbf89-ca95-4ab0-af6a-0bae98508f28",
   "metadata": {
    "tags": []
   },
   "source": [
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340d7d64-89ef-43fa-8da9-6c4cfd5c6ccb",
   "metadata": {},
   "source": [
    "## Parameter Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe8f51f7-3dc6-4d72-97b9-9494f385784c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = easydict.EasyDict(\n",
    "    {\n",
    "        \"experiment\": \"exp1 TimeSformer\",  # 매번 바꿔준다.\n",
    "        \"randomseed\": False,\n",
    "        \"dataset\": \"kids\",  # Options: hmdb51 or ucf101 or `kids`\n",
    "        \"dataset_root_dir\": \"./dataset\",\n",
    "        \"project_dir\": os.getcwd(),\n",
    "        \"model_dir\": \"./pretrained/TimeSformer_divST_96x4_224_K600.pyth\",\n",
    "        \"snapshot\": 20,  # Store a model every snapshot epochs\n",
    "        \"clip_len\": 16,\n",
    "        \"num_workers\": 4,\n",
    "        \"model\": \"TimeSformer\",  # Options: C3D or R2Plus1D or R3D\n",
    "        \"attention_type\": \"divided_space_time\",\n",
    "        \"num_frames\": 19,\n",
    "        \"img_size\": 224,\n",
    "        \"count\": 20,  # how many repeat for wandb\n",
    "        \"epochs\": 10,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"optimizer\": \"adam\",\n",
    "        \"loss_function\": \"focal\",\n",
    "        \"schedular\": \"cosineannealingwarmrestarts\",\n",
    "        \"batch_size\": 20,\n",
    "    }\n",
    ")\n",
    "NAME_ELEMENTS = [args.model, time.strftime(\"%m%d_%H%M\", time.localtime(time.time()))]\n",
    "MODEL_NAME = \"_\".join(NAME_ELEMENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6c96ba-54b1-4adf-a0b7-7c573a000900",
   "metadata": {
    "tags": []
   },
   "source": [
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce28948-bf1e-46f5-80e1-59a3171c8711",
   "metadata": {},
   "source": [
    "## Randomseed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "700f2509-456d-4abe-85e6-93b2bfad33d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if args.randomseed:\n",
    "    torch.manual_seed(args.randomseed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(args.randomseed)\n",
    "    random.seed(args.randomseed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b2630c-ae35-4fab-955b-471ed3945dc6",
   "metadata": {
    "tags": []
   },
   "source": [
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9b9176-d0bb-4eae-8d56-508bc4c4f2ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ce6b8e8-4f96-457f-b8ad-5bae1c3e4dec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device being used: cuda\n"
     ]
    }
   ],
   "source": [
    "# Use GPU if available else revert to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device being used:\", device)\n",
    "\n",
    "if args.dataset == \"hmdb51\":\n",
    "    num_classes = 51\n",
    "elif args.dataset == \"ucf101\":\n",
    "    num_classes = 101\n",
    "elif args.dataset == \"kids\":\n",
    "    num_classes = 9\n",
    "else:\n",
    "    print(\"We only implemented hmdb and ucf datasets.\")\n",
    "    raise NotImplementedError\n",
    "\n",
    "saveName = args.model + \"-\" + args.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c51ef8e-43b7-4f2f-b2f5-2dfcb43fe595",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# build run dir\n",
    "runs = sorted(glob.glob(os.path.join(args.project_dir, \"run\", \"run_*\")))\n",
    "run_id = int(runs[-1].split(\"_\")[-1]) + 1 if runs else 0\n",
    "\n",
    "SAVE_DIR = os.path.join(args.project_dir, \"run\", \"run_\" + str(run_id))\n",
    "model_save_dir = os.path.join(SAVE_DIR, \"models\")\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "if args.model == \"C3D\":\n",
    "    model = C3D_model.C3D(model_dir=model_dir, num_classes=num_classes, pretrained=True)\n",
    "    train_params = [\n",
    "        {\"params\": C3D_model.get_1x_lr_params(model), \"lr\": lr},\n",
    "        {\"params\": C3D_model.get_10x_lr_params(model), \"lr\": lr * 10},\n",
    "    ]\n",
    "\n",
    "elif args.model == \"R2Plus1D\":\n",
    "    model = R2Plus1D_model.R2Plus1DClassifier(\n",
    "        num_classes=num_classes, layer_sizes=(2, 2, 2, 2)\n",
    "    )\n",
    "    train_params = [\n",
    "        {\"params\": R2Plus1D_model.get_1x_lr_params(model), \"lr\": lr},\n",
    "        {\"params\": R2Plus1D_model.get_10x_lr_params(model), \"lr\": lr * 10},\n",
    "    ]\n",
    "\n",
    "elif args.model == \"R3D\":\n",
    "    model = R3D_model.R3DClassifier(num_classes=num_classes, layer_sizes=(2, 2, 2, 2))\n",
    "    train_params = model.parameters()\n",
    "\n",
    "elif args.model == \"TimeSformer\":\n",
    "    model = TimeSformer(\n",
    "        img_size=args.img_size,\n",
    "        num_classes=num_classes,\n",
    "        num_frames=args.num_frames,\n",
    "        attention_type=args.attention_type,\n",
    "        pretrained_model=args.model_dir,\n",
    "    )\n",
    "    train_params = model.parameters()\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "247d8a66-1376-4ac5-89a3-9676f357c79c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training TimeSformer from scratch...\n",
      "Total params: 121.27M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TimeSformer(\n",
       "  (model): VisionTransformer(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (time_drop): Dropout(p=0.0, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (drop_path): DropPath()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (drop_path): DropPath()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (drop_path): DropPath()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (drop_path): DropPath()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (drop_path): DropPath()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (drop_path): DropPath()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (drop_path): DropPath()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (drop_path): DropPath()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (drop_path): DropPath()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (drop_path): DropPath()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (drop_path): DropPath()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (head): Linear(in_features=768, out_features=9, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Training {} from scratch...\".format(args.model))\n",
    "print(\"Total params: %.2fM\" % (sum(p.numel() for p in model.parameters()) / 1000000.0))\n",
    "# model = nn.DataParallel(model, device_ids=[0, 1])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c45ea77-50a9-47fd-97f5-d7a91a69794a",
   "metadata": {
    "tags": []
   },
   "source": [
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794bade0-24b9-4ba7-86b5-36832a4fe055",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset Preprocessing and Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd37c179-0e8a-4a8b-bcf2-bb83af5c586c",
   "metadata": {},
   "source": [
    "* 베이스라인 학습을 위한 데이터 경로 설정\n",
    "\n",
    "원활한 베이스라인 학습을 위하여 `dataset` 디렉토리 안에 다음과 같이 경로를 설정하여 데이터를 저장해야한다.<br>\n",
    "train 데이터셋은 카테고리별로 별도의 디렉토리에 비디오를 분리하여 저장해야 하는데, arrangement.ipynb를 이용하면 바로 분리하여 저장시켜준다.\n",
    "\n",
    "```\n",
    "dataset\n",
    "├── label\n",
    "│   └── kids_labels\n",
    "├── train\n",
    "│   ├── driveway_walk\n",
    "│   │   ├── train_0003.mp4\n",
    "│   │   └── ...\n",
    "│   ├── fall_down\n",
    "│   │   ├── train_0002.mp4\n",
    "│   │   └── ...\n",
    "│   └── fighting\n",
    "│   │   ├── train_0056.mp4\n",
    "│   │   └── ...\n",
    "│   ...\n",
    "└── test (공개가 안되어있다)\n",
    "    ├── test_0000.mp4\n",
    "    ├── test_0000.mp4\n",
    "    ├── test_0000.mp4\n",
    "    ├── ...\n",
    "```\n",
    "\n",
    "학습 및 추론 전처리 과정에서 각각 train_processed, test_processed 디렉토리가 다음과 같이 생성된다.<br>\n",
    "이는 dataset.py를 실행시키면 되나, 굳이 할 필요는 없다.<br>\n",
    "전처리에서는 비디오에서 16프레임을 샘플링한 이미지 데이터를 비디오 파일명 디렉토리에 별도로 저장하는 과정이 수행된다.\n",
    "```\n",
    "dataset\n",
    "├── label\n",
    "│   └── kids_labels\n",
    "├── train\n",
    "│   ├── driveway_walk\n",
    "│   │   ├── train_0003.mp4\n",
    "│   │   └── ...\n",
    "│   ├── fall_down\n",
    "│   │   ├── train_0002.mp4\n",
    "│   │   └── ...\n",
    "│   └── fighting\n",
    "│   │   ├── train_0056.mp4\n",
    "│   │   └── ...\n",
    "│   ...\n",
    "│\n",
    "├── train_processed\n",
    "│   ├── train\n",
    "│   │   ├── driveway_walk\n",
    "│   │   │   ├── train_0003\n",
    "│   │   │   │   ├── 00000.jpg\n",
    "│   │   │   │   ├── 00001.jpg\n",
    "│   │   │   └── ...\n",
    "│   │   └── ...\n",
    "│   └── val\n",
    "│   │   ├── driveway_walk\n",
    "│   │   │   ├── train_0004\n",
    "│   │   │   │   ├── 00000.jpg\n",
    "│   │   │   │   ├── 00001.jpg\n",
    "│   │   │   │   └── ...\n",
    "│   │   │   └── ...\n",
    "│   │   └── ...\n",
    "│\n",
    "├── test\n",
    "│   ├── test_0000.mp4\n",
    "│   ├── test_0000.mp4\n",
    "│   ├── test_0000.mp4\n",
    "│   └── ...\n",
    "│\n",
    "└── test_processed\n",
    "    ├── test_0000\n",
    "    │   ├── 00000.jpg\n",
    "    │   ├── 00001.jpg\n",
    "    │   └── ...\n",
    "    ├── test_0001\n",
    "    │   ├── 00000.jpg\n",
    "    │   ├── 00001.jpg\n",
    "    │   └── ...\n",
    "    ├── ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90ce7ae9-2137-4d19-a631-5787f24c88f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model on <module 'source.dataset' from '/home/stephencha/Hub/soo/source/dataset.py'> dataset...\n",
      "Number of train videos: 2663\n",
      "Number of val videos: 670\n"
     ]
    }
   ],
   "source": [
    "print(\"Training model on {} dataset...\".format(dataset))\n",
    "train_dataset = dataset.VideoDataset(\n",
    "    root_dir=args.dataset_root_dir,\n",
    "    dataset=args.dataset,\n",
    "    split=\"train\",\n",
    "    clip_len=args.clip_len,\n",
    ")\n",
    "val_dataset = dataset.VideoDataset(\n",
    "    root_dir=args.dataset_root_dir,\n",
    "    dataset=args.dataset,\n",
    "    split=\"val\",\n",
    "    clip_len=args.clip_len,\n",
    ")\n",
    "\n",
    "\n",
    "def build_dataset(batch_size):\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=args.num_workers\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset, batch_size=batch_size, num_workers=args.num_workers\n",
    "    )\n",
    "\n",
    "    trainval_loaders = {\"train\": train_dataloader, \"val\": val_dataloader}\n",
    "    trainval_sizes = {x: len(trainval_loaders[x].dataset) for x in [\"train\", \"val\"]}\n",
    "\n",
    "    return trainval_loaders, trainval_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74126a81-da0d-419e-b924-0ff198b6a951",
   "metadata": {
    "tags": []
   },
   "source": [
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337e70e4-9c38-4d4b-975b-91ba637fa983",
   "metadata": {},
   "source": [
    "## Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb49a0ed-0ec0-4ef2-8b58-d38db387dfc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import timm.optim.nadam as nadam\n",
    "import torchcontrib\n",
    "from source.focalloss import FocalLoss\n",
    "from source.label_smooth import LabelSmoothSoftmaxCEV2\n",
    "from torch.optim.swa_utils import SWALR\n",
    "from torchcontrib.optim import SWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03973808-9792-4d7c-9273-24380a4fc0a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_loss_function(lf):\n",
    "    if lf == \"focal\":\n",
    "        lf = FocalLoss()\n",
    "    elif lf == \"cross_entropy\":\n",
    "        lf = nn.CrossEntropyLoss()\n",
    "    elif lf == \"label_smooth\":\n",
    "        lf = LabelSmoothSoftmaxCEV2()\n",
    "    return lf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eedbb057-8ca2-483b-89e0-0fc6fafa9300",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-10-30b0769fb2dd>, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-30b0769fb2dd>\"\u001b[0;36m, line \u001b[0;32m17\u001b[0m\n\u001b[0;31m    raise NotImplementedError\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "def build_optimizer(model, opt, lr):\n",
    "    if args.model == \"C3D\":\n",
    "        param = [\n",
    "            {\"params\": C3D_model.get_1x_lr_params(model), \"lr\": lr},\n",
    "            {\"params\": C3D_model.get_10x_lr_params(model), \"lr\": lr * 10},\n",
    "        ]\n",
    "    elif args.model == \"R2Plus1D\":\n",
    "        param = [\n",
    "            {\"params\": R2Plus1D_model.get_1x_lr_params(model), \"lr\": lr},\n",
    "            {\"params\": R2Plus1D_model.get_10x_lr_params(model), \"lr\": lr * 10},\n",
    "        ]\n",
    "    elif args.model == \"R3D\":\n",
    "        param = model.parameters()\n",
    "    elif args.model == \"TimeSformer\":\n",
    "        param = model.parameters()\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    if opt == \"sgd\":\n",
    "        optimizer = optim.SGD(param, lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "    elif opt == \"adam\":\n",
    "        optimizer = optim.Adam(param, lr=lr, amsgrad=True)\n",
    "    elif opt == \"adamw\":\n",
    "        optimizer = optim.AdamW(param, lr=lr)\n",
    "    elif opt == \"adadelta\":\n",
    "        optimizer = optim.Adadelta(param, lr=lr)\n",
    "    elif opt == \"nadam\":\n",
    "        optimizer = nadam.Nadam(param, lr=lr)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241305d1-de1f-4624-95a7-9305767a59cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_schedular(optimizer, sche, epochs, length):\n",
    "    if sche == \"step\":\n",
    "        schedular = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif sche == \"onecycle\":\n",
    "        schedular = optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            pct_start=0.1,\n",
    "            div_factor=1e5,\n",
    "            max_lr=0.0001,\n",
    "            epochs=epochs,\n",
    "            steps_per_epoch=length,\n",
    "        )\n",
    "    elif sche == \"cosineannealingwarmrestarts\":\n",
    "        schedular = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer, T_0=10, T_mult=2, eta_min=1e-5, last_epoch=-1\n",
    "        )\n",
    "    elif sche == \"swa\":\n",
    "        schedular = SWALR(optimizer, swa_lr=0.01)\n",
    "    return schedular"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939bf330-61d0-43f2-916f-d33ffa9846f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33282196-0833-4e3c-bdd4-cbc6b2aece55",
   "metadata": {},
   "source": [
    "## Train\n",
    "* epoch마다 학습과 검증을 실시한다.\n",
    "* `./run/run_*` 디렉토리에서 저장된 가중치 파일을 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ea3eba-c69c-4cd7-8761-e52ed17a7c16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainval_loaders, trainval_sizes = build_dataset(args.batch_size)\n",
    "\n",
    "# standard crossentropy loss for classification\n",
    "criterion = build_loss_function(args.loss_function)\n",
    "optimizer = build_optimizer(model, opt=args.optimizer, lr=args.learning_rate)\n",
    "\n",
    "# the scheduler divides the lr by 10 every 10 epochs\n",
    "if args.schedular == \"swa\":\n",
    "    optimizer = torchcontrib.optim.SWA(optimizer)\n",
    "\n",
    "scheduler = build_schedular(\n",
    "    optimizer,\n",
    "    sche=args.schedular,\n",
    "    epochs=args.epochs,\n",
    "    length=len(trainval_loaders[\"train\"]),\n",
    ")\n",
    "\n",
    "best_score = 0  # np.Inf\n",
    "for epoch in range(args.epochs):\n",
    "    # each epoch has a training and validation step\n",
    "    for phase in [\"train\", \"val\"]:\n",
    "        start_time = timeit.default_timer()\n",
    "\n",
    "        # reset the running loss and corrects\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0.0\n",
    "\n",
    "        # set model to train() or eval() mode depending on whether it is trained\n",
    "        # or being validated. Primarily affects layers such as BatchNorm or Dropout.\n",
    "        if phase == \"train\":\n",
    "            # scheduler.step() is to be called once every epoch during training\n",
    "            scheduler.step()\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        epoch_labels = []\n",
    "        epoch_preds = []\n",
    "\n",
    "        for inputs, labels in tqdm(trainval_loaders[phase]):\n",
    "            # move inputs and labels to the device the training is taking place on\n",
    "            inputs = Variable(inputs, requires_grad=True).to(device)\n",
    "            labels = Variable(labels).to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if phase == \"train\":\n",
    "                outputs = model(inputs)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(inputs)\n",
    "\n",
    "            probs = nn.Softmax(dim=1)(outputs)\n",
    "            preds = torch.max(probs, 1)[1]\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            if phase == \"train\":\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_labels.extend(labels.tolist())\n",
    "            epoch_preds.extend(preds.tolist())\n",
    "\n",
    "        epoch_loss = running_loss / trainval_sizes[phase]\n",
    "        epoch_acc = running_corrects.double() / trainval_sizes[phase]\n",
    "\n",
    "        epoch_score = f1_score(epoch_preds, epoch_labels, average=\"weighted\")\n",
    "        print(f\"{phase} | EPOCH {epoch} Weighted F1 SCORE: {epoch_score}\")\n",
    "\n",
    "        print(\n",
    "            \"[{}] Epoch: {}/{} Loss: {} Acc: {}\".format(\n",
    "                phase, epoch + 1, args.epochs, epoch_loss, epoch_acc\n",
    "            )\n",
    "        )\n",
    "        stop_time = timeit.default_timer()\n",
    "        print(\"Execution time: \" + str(stop_time - start_time) + \"\\n\")\n",
    "\n",
    "        if epoch_score > best_score:\n",
    "            print(\n",
    "                f\"Validation Weighted F1 Score increased ({best_score:.6f} --> {epoch_score:.6f}).  Saving model ...\"\n",
    "            )\n",
    "            model_path = os.path.join(\n",
    "                SAVE_DIR,\n",
    "                \"models\",\n",
    "                saveNamedecreased\n",
    "                + \"_epoch-\"\n",
    "                + str(epoch)\n",
    "                + \"_epoch_score-{:.6f}.pt\".format(epoch_score)\n",
    "                + \".pth.tar\",\n",
    "            )\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch + 1,\n",
    "                    \"state_dict\": model.state_dict(),\n",
    "                    \"opt_dict\": optimizer.state_dict(),\n",
    "                },\n",
    "                model_path,\n",
    "            )\n",
    "            print(\"Save model at {}\\n\".format(model_path))\n",
    "            best_score = epoch_score\n",
    "            # path_dir = [SAVE_DIR, \"{:.6f}.pt\".format(epoch_score)]\n",
    "            # torch.save(model.state_dict(), os.path.join(*path_dir))\n",
    "\n",
    "    \"\"\"if epoch % args.save_epoch == (args.save_epoch - 1):\n",
    "        model_path = os.path.join(\n",
    "            SAVE_DIR, \"models\", saveName + \"_epoch-\" + str(epoch) + \".pth.tar\"\n",
    "        )\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"state_dict\": model.state_dict(),\n",
    "                \"opt_dict\": optimizer.state_dict(),\n",
    "            },\n",
    "            model_path,\n",
    "        )\n",
    "        print(\"Save model at {}\\n\".format(model_path))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0188bc83-1b3c-4683-8361-524e183600e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9d9c47-78f9-4f29-bd08-08f437cbb905",
   "metadata": {
    "tags": []
   },
   "source": [
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aed22a-7df4-4978-8ad1-0409f74e3273",
   "metadata": {},
   "source": [
    "## Inference\n",
    "* 루트 디렉토리에 생성된 `submit.csv` 파일을 확인하고, 제출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc1b58c-cd16-49f9-8457-00137f97cb5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cls_li = [\n",
    "    \"driveway_walk\",\n",
    "    \"fall_down\",\n",
    "    \"fighting\",\n",
    "    \"jay_walk\",\n",
    "    \"normal\",\n",
    "    \"putup_umbrella\",\n",
    "    \"ride_cycle\",\n",
    "    \"ride_kick\",\n",
    "    \"ride_moto\",\n",
    "]\n",
    "\n",
    "DATA_DIR = args.dataset_root_dir  # os.path.join(PROJECT_DIR, '')\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d96e421-c572-4ba1-b588-9163dc9ea749",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\n",
    "    model_path, map_location=lambda storage, loc: storage\n",
    ")  # Load all tensors onto the CPU\n",
    "print(f\"Initializing weights from: {model_path.split('/')[-1]}...\")\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "print(\"Total params: %.2fM\" % (sum(p.numel() for p in model.parameters()) / 1000000.0))\n",
    "\n",
    "print(\"Model Inference on {} dataset...\".format(dataset))\n",
    "\n",
    "if os.path.isdir(os.path.join(DATA_DIR, \"test_processed\")):\n",
    "    preprocess = False\n",
    "else:\n",
    "    preprocess = True\n",
    "\n",
    "test_dataset = dataset.TestDataset(root_dir=args.dataset_root_dir, dataset=dataset, clip_len=16, preprocess=preprocess)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "model.eval()\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "pred_li = []\n",
    "for inputs in tqdm(test_dataloader):\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "\n",
    "    probs = nn.Softmax(dim=1)(outputs)\n",
    "    preds = torch.max(probs, 1)[1]\n",
    "    pred_li.extend(preds.tolist())\n",
    "\n",
    "stop_time = timeit.default_timer()\n",
    "print(\"Execution time: \" + str(stop_time - start_time) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41e3be6-5edf-451b-9128-b3d541565bca",
   "metadata": {
    "tags": []
   },
   "source": [
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64bfa86-4dac-4a81-9414-2f4a8a8bea71",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe9ee17-ec0a-401e-a295-f00dc8d8f316",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"sample_submission.csv\")\n",
    "sample_submission[\"class\"] = [cls_li[int(pred)] for pred in pred_li]\n",
    "sample_submission.to_csv(\"submit_{}.csv\".format(model_path.split('/')[-1]), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37631998-b59e-4009-b16b-f50b855ae6a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9919bac1-984f-4fed-841f-4f60f2471595",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34347890-f4dc-4055-bde2-a6d489794f07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aacdf25-e089-436a-911c-8a1652463ac6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
